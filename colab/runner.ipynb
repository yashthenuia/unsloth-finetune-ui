{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQUzAOjLfA6EBtieCjSius",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashthenuia/unsloth-finetune-ui/blob/main/colab/runner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjLwkHPpMH17"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth trl accelerate datasets bitsandbytes\n",
        "print(\"Unsloth installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yashthenuia/unsloth-finetune-ui.git\n",
        "%cd unsloth-finetune-ui/colab\n"
      ],
      "metadata": {
        "id": "Yz36htG3MR-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "DATA_FILE = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", DATA_FILE)\n"
      ],
      "metadata": {
        "id": "vvPVT6jOPERR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"MODEL_CHOICE\"] = \"unsloth/mistral-7b\"\n",
        "os.environ[\"EPOCHS\"] = \"2\"\n",
        "os.environ[\"BATCH_SIZE\"] = \"2\"\n",
        "os.environ[\"LR\"] = \"0.0002\"\n",
        "os.environ[\"DATA_FILE\"] = DATA_FILE\n",
        "\n",
        "!python3 trainer.py\n"
      ],
      "metadata": {
        "id": "Lw3-Uxj-Xcyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your Trained model"
      ],
      "metadata": {
        "id": "r87oZXReXprQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAPjAskBXctC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments"
      ],
      "metadata": {
        "id": "k6yTBGZGXcpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"customer_support_model\",\n",
        "    max_seq_length=2048,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def ask_question(question):\n",
        "    \"\"\"Generate response to a customer question.\"\"\"\n",
        "    if \"mistral\" in model_name.lower():\n",
        "        prompt = f\"<s>[INST] You are a helpful customer support agent. Answer clearly and professionally.\\n\\n{question} [/INST]\"\n",
        "    elif \"llama\" in model_name.lower():\n",
        "        prompt = f\"\"\"system\n",
        "You are a helpful customer support agent. Answer clearly and professionally.\n",
        "user\n",
        "{question}\n",
        "assistant\"\"\"\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"### Instruction:\\nYou are a helpful customer support agent. \"\n",
        "            \"Answer clearly and professionally.\\n\\n### Input:\\n\"\n",
        "            f\"{question}\\n\\n### Response:\"\n",
        "        )\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract just the response text\n",
        "    if \"[/INST]\" in response:\n",
        "        response = response.split(\"[/INST]\")[-1].strip()\n",
        "    elif \"assistant\" in response:\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "    elif \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[-1].strip()\n",
        "    return response"
      ],
      "metadata": {
        "id": "j911W5iUXclI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/mistral-7b\"\n",
        "test_questions = [\n",
        "    \"How do I reset my password?\",\n",
        "    \"My payment was declined\",\n",
        "    \"How do I cancel my subscription?\",\n",
        "    \"The app keeps crashing\",\n",
        "    \"Where is my order?\",\n",
        "    \"Do you offer student discounts?\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CUSTOMER SUPPORT AGENT TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n Customer {i}: {question}\")\n",
        "    answer = ask_question(question)\n",
        "    print(f\"Support Agent: {answer}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\nTesting complete!\")"
      ],
      "metadata": {
        "id": "LUuL_8gXXceH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}